['Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine']
['Learning, pages 1638–1646. PMLR, 2014.']
['Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. In International']
['Conference on Machine Learning, pages 551–560. PMLR, 2020.']
['Yu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. arXiv preprint arXiv:2006.12007, 2020.']
['Noam Brown and Tuomas Sandholm. Regret transfer and parameter optimization. In Workshops at the']
['Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014.']
['Noam Brown and Tuomas Sandholm. Regret-based pruning in extensive-form games. In NIPS, pages 1972–']
['1980, 2015.']
['Noam Brown and Tuomas Sandholm. Reduced space and faster convergence in imperfect-information games via pruning. In International conference on machine learning, pages 596–604. PMLR, 2017.']
['Noam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats top professionals. Science, 359(6374):418–424, 2018.']
['Noam Brown, Christian Kroer, and Tuomas Sandholm. Dynamic thresholding and pruning for regret minimization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31, 2017.']
['Noam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm. Deep counterfactual regret minimization.']
['In International conference on machine learning, pages 793–802. PMLR, 2019.']
['Neil Burch, Matej Moravcik, and Martin Schmid. Revisiting cfr+ and alternating updates. Journal of']
['Artificial Intelligence Research, 64:429–443, 2019.']
['Andrea Celli, Stefano Coniglio, and Nicola Gatti. Computing optimal ex ante correlated equilibria in twoplayer sequential games. In Proceedings of the 18th International Conference on Autonomous Agents and']
['MultiAgent Systems, pages 909–917, 2019.']
['Nicolo Cesa-Bianchi and Gábor Lugosi. Prediction, learning, and games. Cambridge university press, 2006.']
['Zixiang Chen, Dongruo Zhou, and Quanquan Gu. Almost optimal algorithms for two-player markov games with linear function approximation. arXiv preprint arXiv:2102.07404, 2021.']
['Constantinos Daskalakis, Dylan J Foster, and Noah Golowich. Independent policy gradient methods for competitive reinforcement learning. Advances in neural information processing systems, 33:5527–5540, 2020.']
['Gabriele Farina and Tuomas Sandholm. Model-free online learning in unknown sequential decision making problems and games. arXiv preprint arXiv:2103.04539, 2021.']
['Gabriele Farina, Tommaso Bianchi, and Tuomas Sandholm. Coarse correlation in extensive-form games. In']
['Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 1934–1941, 2020a.']
['Gabriele Farina, Christian Kroer, and Tuomas Sandholm. Faster game solving via predictive blackwell approachability: Connecting regret matching and mirror descent. arXiv preprint arXiv:2007.14358, 2020b.']
['Gabriele Farina, Christian Kroer, and Tuomas Sandholm. Stochastic regret minimization in extensive-form games. In International Conference on Machine Learning, pages 3018–3028. PMLR, 2020c.']
['Gabriele Farina, Robin Schmucker, and Tuomas Sandholm. Bandit linear optimization for sequential decision making and extensive-form games. arXiv preprint arXiv:2103.04546, 2021.']
['Richard Gibson, Marc Lanctot, Neil Burch, Duane Szafron, and Michael Bowling. Generalized sampling and variance in counterfactual regret minimization. In Proceedings of the AAAI Conference on Artificial']
['Intelligence, volume 26, 2012a.']
['Richard G Gibson, Neil Burch, Marc Lanctot, and Duane Szafron. Efficient monte carlo counterfactual regret minimization in games with many player actions. In NIPS, pages 1889–1897, 2012b.']
['Andrew Gilpin, Javier Pena, and Tuomas W Sandholm. First-order algorithm with o (ln (1/ε)) convergence for-equilibrium in two-person zero-sum games. 2008.']
['Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. Econometrica, 68(5):1127–1150, 2000.']
['Johannes Heinrich, Marc Lanctot, and David Silver. Fictitious self-play in extensive-form games. In International conference on machine learning, pages 805–813. PMLR, 2015.']
['Samid Hoda, Andrew Gilpin, Javier Pena, and Tuomas Sandholm. Smoothing techniques for computing nash equilibria of sequential games. Mathematics of Operations Research, 35(2):494–512, 2010.']
['Baihe Huang, Jason D Lee, Zhaoran Wang, and Zhuoran Yang. Towards general function approximation in zero-sum markov games. arXiv preprint arXiv:2107.14702, 2021.']
['Chi Jin, Qinghua Liu, Yuanhao Wang, and Tiancheng Yu. V-learning–a simple, efficient, decentralized algorithm for multiagent rl. arXiv preprint arXiv:2110.14555, 2021a.']
['Chi Jin, Qinghua Liu, and Tiancheng Yu. The power of exploiter: Provable multi-agent rl in large state spaces. arXiv preprint arXiv:2106.03352, 2021b.']
['Michael Johanson. Measuring the size of large no-limit poker games. arXiv preprint arXiv:1302.7008, 2013.']
['Michael Johanson, Nolan Bard, Marc Lanctot, Richard G Gibson, and Michael Bowling. Efficient nash equilibrium approximation through monte carlo counterfactual regret minimization. In AAMAS, pages']
['837–846. Citeseer, 2012.']
['Daphne Koller and Nimrod Megiddo. The complexity of two-person zero-sum games in extensive form.']
['Games and economic behavior, 4(4):528–552, 1992.']
['Daphne Koller, Nimrod Megiddo, and Bernhard Von Stengel. Efficient computation of equilibria for extensive two-person games. Games and economic behavior, 14(2):247–259, 1996.']
['Tadashi Kozuno, Pierre Ménard, Rémi Munos, and Michal Valko. Model-free learning for two-player zero-sum partially observable markov games with perfect recall. arXiv preprint arXiv:2106.06279, 2021.']
['Christian Kroer, Kevin Waugh, Fatma Kilinç-Karzan, and Tuomas Sandholm. Faster first-order methods for extensive-form game solving. In Proceedings of the Sixteenth ACM Conference on Economics and']
['Computation, pages 817–834, 2015.']
['Christian Kroer, Gabriele Farina, and Tuomas Sandholm. Solving large sequential games with the excessive gap technique. arXiv preprint arXiv:1810.03063, 2018.']
['Harold W Kuhn. Extensive games and the problem of information. In Contributions to the Theory of Games']
['(AM-28), Volume II, pages 193–216. Princeton University Press, 1953.']
['Marc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael H Bowling. Monte carlo sampling for regret minimization in extensive games. In NIPS, pages 1078–1086, 2009.']
['Tor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University Press, 2020.']
['Chung-Wei Lee, Christian Kroer, and Haipeng Luo. Last-iterate convergence in extensive-form games.']
['Advances in Neural Information Processing Systems, 34, 2021.']
['Viliam Lisỳ, Marc Lanctot, and Michael H Bowling. Online monte carlo counterfactual regret minimization for search in imperfect information games. In AAMAS, pages 27–36, 2015.']
['Qinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforcement learning with self-play. In International Conference on Machine Learning, pages 7001–7010. PMLR, 2021.']
['Weichao Mao and Tamer Başar. Provably efficient reinforcement learning in decentralized general-sum markov games. Dynamic Games and Applications, pages 1–22, 2022.']
['Matej Moravčı́k, Martin Schmid, Neil Burch, Viliam Lisỳ, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin']
['Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science, 356(6337):508–513, 2017.']
['Remi Munos, Julien Perolat, Jean-Baptiste Lespiau, Mark Rowland, Bart De Vylder, Marc Lanctot, Finbarr']
['Timbers, Daniel Hennes, Shayegan Omidshafiei, Audrunas Gruslys, et al. Fast computation of nash equilibria in imperfect information games. In International Conference on Machine Learning, pages 7119–']
['7129. PMLR, 2020.']
['John F Nash. Equilibrium points in n-person games. Proceedings of the National Academy of Sciences of the United States of America, 36(1):48–49, 1950.']
['Gergely Neu. Explore no more: Improved high-probability regret bounds for non-stochastic bandits. arXiv preprint arXiv:1506.03271, 2015.']
['Martin Schmid, Neil Burch, Marc Lanctot, Matej Moravcik, Rudolf Kadlec, and Michael Bowling. Variance reduction in monte carlo counterfactual regret minimization (vr-mccfr) for extensive form games using baselines. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 2157–2164, 2019.']
['Martin Schmid, Matej Moravcik, Neil Burch, Rudolf Kadlec, Josh Davidson, Kevin Waugh, Nolan Bard, Finbarr Timbers, Marc Lanctot, Zach Holland, et al. Player of games. arXiv preprint arXiv:2112.03178, 2021.']
['Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10):1095–1100, 1953.']
['Aaron Sidford, Mengdi Wang, Lin Yang, and Yinyu Ye. Solving discounted stochastic two-player games with near-optimal time and sample complexity. In International Conference on Artificial Intelligence and']
['Statistics, pages 2992–3002. PMLR, 2020.']
['Ziang Song, Song Mei, and Yu Bai. When can we learn general-sum markov games with a large number of players sample-efficiently? arXiv preprint arXiv:2110.04184, 2021.']
['Oskari Tammelin. Solving large imperfect information games using cfr+. arXiv preprint arXiv:1407.5042, 2014.']
['Yuandong Tian, Qucheng Gong, and Tina Jiang. Joint policy search for multi-agent collaboration with imperfect information. arXiv preprint arXiv:2008.06495, 2020.']
['Bernhard Von Stengel. Efficient computation of behavior strategies. Games and Economic Behavior, 14(2):']
['220–246, 1996.']
['Kevin Waugh, Dustin Morrill, James Andrew Bagnell, and Michael Bowling. Solving games with functional regret estimation. In Twenty-ninth AAAI conference on artificial intelligence, 2015.']
['Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Online reinforcement learning in stochastic games. arXiv preprint arXiv:1712.00579, 2017.']
['Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Last-iterate convergence of decentralized optimistic gradient descent/ascent in infinite-horizon competitive markov games. In Conference on']
['Learning Theory, pages 4259–4299. PMLR, 2021.']
['Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneous-move markov games using function approximation and correlated equilibrium. In Conference on Learning Theory, pages 3674–3682. PMLR, 2020.']
['Brian Hu Zhang and Tuomas Sandholm. Finding and certifying (near-) optimal strategies in black-box extensive-form games. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages']
['5779–5788, 2021.']
['Kaiqing Zhang, Sham M Kakade, Tamer Başar, and Lin F Yang. Model-based multi-agent rl in zero-sum markov games with near-optimal sample complexity. arXiv preprint arXiv:2007.07461, 2020.']
['Yichi Zhou, Jialian Li, and Jun Zhu. Posterior sampling for multi-agent reinforcement learning: solving extensive games with imperfect information. In International Conference on Learning Representations, 2019.']
['Yichi Zhou, Tongzheng Ren, Jialian Li, Dong Yan, and Jun Zhu. Lazy-cfr: fast and near-optimal regret minimization for extensive games with imperfect information. In International Conference on Learning']
['Representations, 2020. URL https://openreview.net/forum?id=rJx4p3NYDB.']
['Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization in games with incomplete information. Advances in neural information processing systems, 20:1729–1736, 2007.']
['A Technical tools']
['The following Freedman’s inequality can be found in (Agarwal et al., 2014, Lemma 9).']
['Lemma A.1 (Freedman’s inequality). Suppose random variables {Xt}']
['T t=1 is a martingale difference sequence, i.e. Xt ∈ Ft where {Ft}t≥1 is a filtration, and E[Xt|Ft−1] = 0. Suppose Xt ≤ R almost surely for some (non-random) R > 0. Then for any λ ∈ (0, 1/R], we have with probability at least 1 − δ that']
['T']
['X t=1']
['Xt ≤ λ ·']
['E']
['\x02']
['X2 t |Ft−1']
['\x03']
['+ log(1/δ)']
['λ']
['.']
['B Bounds for regret minimizers']
['Here we collect regret bounds for various regret minimization algorithms on the probability simplex. For any algorithm that plays policy pt in the t-th round and observes loss vector {ℓt(a)}a∈[A] ∈ RA']
['≥0, define its regret as']
['Regret(T ) := max p⋆∈∆([A])']
['D pt, e']
['ℓt']
['−']
['D p⋆']
[', e']
['B.1 Hedge']
['Algorithm 3 Regret Minimization with Hedge (Hedge)']
['Require: Learning rate η > 0.']
['1: Initialize p1(a) ← 1/A for all a ∈ [A].']
['2: for iteration t = 1, . . . , T do']
['3: Receive loss vector n e']
['ℓt(a) o a∈[A]']
['4: Update action distribution via mirror descent: pt+1(a) ∝a pt(a) exp']
['\x10']
['−ηe']
['ℓt(a)']
['\x11']
['The following regret bound for Hedge is standard, see, e.g. (Lattimore and Szepesvári, 2020, Proposition']
['28.7).']
['Lemma B.1 (Regret bound for Hedge). Algorithm 3 with learning rate η > 0 achieves regret bound']
['Regret(T ) ≤ log A']
['η']
['+']
['2']
['X a∈[A] pt(a)e']
['ℓt(a)2']
['B.2 Regret Matching']
['The following regret bound for Regret Matching is standard, see, e.g. (Cesa-Bianchi and Lugosi, 2006;']
['Brown and Sandholm, 2014). For completeness, here we provide a proof along with an alternative form of bound useful for our purpose (Remark B.3). Note that here η is not the learning rate but rather an arbitrary positive value (i.e. the right-hand side is an upper bound on the regret for any η > 0). Algorithm 4 itself does not require any learning rate.']
['Lemma B.2 (Regret bound for Regret Matching). Algorithm 4 achieves the following regret bound for any']
['η > 0:']
['Regret(T ) ≤ h T']
['X a∈[A]']
['\x10 D pt, e']
['− e']
['\x112i1/2']
['≤']
['1']
['4']
['\x112']
['Algorithm 4 Regret Minimization with Regret Matching (RegretMatching)']
['1: Initialize p1(a) ← 1/A and R0(a) ← 0 for all a ∈ [A].']
['4: Update instantaneous regret and cumulative regret for all a ∈ [A]: rt(a) ←']
['ℓt(a) and Rt(a) ← Rt−1(a) + rt(a).']
['5: Compute action distribution by regret matching: pt+1(a) ←']
['[Rt(a)]+']
['P a′∈[A] [Rt(a′)]+']
['= hPT t=1']
['D pt, ℓ̃t']
['ℓt(a) i']
['P a′∈[A] hPT t=1']
['ℓt(a′) i']
['In the edge case where [Rt(a)]+ = 0 for all a ∈ [A], set pt+1(a) ← 1/A to be the uniform distribution.']
['Proof. By the fact that (a + b)2']
['+ ≤ a2']
['+ + 2a+b + b2']
[', we have']
['[Rt(a)]2']
['+ ≤ [Rt−1(a)]2']
['+ + 2[Rt−1(a)]+rt(a) + rt(a)2']
['. (14)']
['Then by the definition of pt(a) and rt(a), we have']
['[Rt−1(a)]+rt(a) =']
['[Rt−1(a)]+']
['\x10 X a′∈[A] pt(a′']
[')e']
['ℓt(a′']
[') − e']
['=']
['[Rt−1(a)]+ e']
['ℓt(a) −']
['ℓt(a) = 0.']
['(15)']
['Then summing over a in Eq. (14) and using Eq. (15), we get']
['[RT (a)]2']
['+ ≤']
['[RT −1(a)]2']
['+ + 2']
['[RT −1(a)]+rT (a) +']
['X a∈[A] rT (a)2']
['+ +']
['X a∈[A] rt(a)2']
['Using that maxa RT (a) ≤ maxa[RT (a)]+ ≤ (']
['P a∈[A][RT (a)]2']
['+)1/2 gives the regret bound']
['Regret(T ) = max a∈[A]']
['RT (a) ≤']
['\uf8eb']
['\uf8ed']
['\uf8f6']
['\uf8f8']
['1/2']
['\x10D pt, e']
['The claimed bound with η follows directly from the inequality']
['√ z ≤ 1/η + ηz/4 for any η > 0, z ≥ 0.']
['Remark B.3. The quantity']
['P a∈[A]']
['\x112 above can be upper bounded as']
['\x12D pt, e']
['E2']
['+ e']
['\x13']
['= A']
['≤ A']
['\x10 pt(a)e']
['+ (1/A)e']
['= 2A']
['X a∈[A] p̄t(a)e']
[', where p̄t(a) = [pt(a) + (1/A)]/2 is a probability distribution over [A].']
['As a consequence, we get an upper bound on the regret of Regret Matching algorithm by']
['Regret(T ) ≤']
['(Ap̄t(a))e']
['Comparing to the bound of Hedge (Lemma B.1), the above regret bound for Regret Matching has a similar form except for replacing log A by 1 and replacing pt by Ap̄t.']
['C Properties of the game']
['C.1 Basic properties']
['For any opponent (min-player) policy ν ∈ Πmin, define pν']
['1:h(xh) :=']
['X sh∈xh p1:h(sh)ν1:h−1(y(sh−1), bh−1) for all h ∈ [H], xh ∈ Xh.']
['Intuitively, pν']
['1:h(xh) measures the environment and the opponent’s contribution in the reaching probability of xh.']
['Lemma C.1 (Properties of pν']
['1:h(xh)). The following holds for any ν ∈ Πmin:']
['(a) For any policy µ ∈ Πmax, we have']
['X']
['(xh,ah)∈Xh×A']
['µ1:h(xh, ah)pν']
['1:h(xh) = 1.']
['(b) 0 ≤ pν']
['1:h(xh) ≤ 1 for all h, xh.']
['Proof. For (a), notice that']
['1:h(xh) =']
['X sh∈xh p1:h(sh) · µ1:h(xh, ah) · ν1:h−1(y(sh−1), bh−1)']
['X sh∈xh']
['Pµ,ν']
['(visit (sh, ah)) = Pµ,ν']
['(visit (xh, ah)).']
['Summing over all (xh, ah) ∈ Xh × A, the right hand side sums to one, thereby showing (a).']
['For (b), fix any xh ∈ Xh. Clearly pν']
['1:h(xh) ≥ 0. Choose any ah ∈ A, and choose policy µxh,ah']
['∈ Πmax such that µxh,ah']
['1:h (xh, ah) = 1 (such µxh,ah exists, for example, by deterministically taking all actions prescribed in infoset xh at all ancestors of xh). For this µxh,ah']
[', using (a), we have pν']
['1:h(xh) = µxh,ah']
['1:h (xh, ah) · pν']
['1:h(xh) ≤']
['(x′ h,a′ h)∈Xh×A']
['µxh,ah']
['1:h (x′ h, a′ h) · pν']
['1:h(x′ h) = 1.']
['This shows part (b).']
['Corollary C.2. For any policy µ ∈ Πmax and h ∈ [H], we have']
['µ1:h(xh, ah)ℓt h(xh, ah) ≤ 1.']
['Proof. Notice by definition']
['ℓt h(xh, ah) =']
['X sh∈xh,bh∈Bh p1:h(sh)νt']
['1:h(y(sh), bh)(1 − rh(sh, ah, bh)) ≤ pν']
['1:h(xh), and the result is implied by Lemma C.1 (b).']
['Lemma C.3. For any h ∈ [H], the counterfactual loss function Lt h defined in (10) satisfies the bound']
['µ1:h(xh, ah)Lt h(xh, ah) ≤ H − h + 1.']
['(b) For any (h, xh, ah), we have']
['0 ≤ Lt h(xh, ah) ≤ pνt']
['1:h(xh) · (H − h + 1).']
['Proof. Part (a) follows from the fact that']
['µ1:h(xh, ah)Lt h(xh, ah) = Eµ,νt']
['" H']
['X h′=h rh′']
['#']
['≤ H − h + 1, where the first equality follows from the definition of the loss functions ℓh and Lh in (3), (10).']
['For part (b), the nonnegativity follows clearly by definition. For the upper bound, take any policy µxh,ah']
['∈']
['Πmax such that µxh,ah']
['1:h (xh, ah) = 1. We then have']
['Lt h(xh, ah) = µxh,ah']
['1:h (xh, ah)Lt h(xh, ah) = Eµxh,ah ,νt']
['"']
['1 {visit xh, ah} ·']
['H']
['= Pµxh,ah ,νt (visit xh, ah) · Eµxh,ah ,νt']
['X h′=h rh′ visit xh, ah']
['≤ µxh,ah']
['1:h (xh, ah)pνt']
['1:h(xh) · (H − h + 1) = pνt']
['Definition of average policies For two-player zero-sum IIEFGs, we define the average policy of the max-player µ = 1']
['PT t=1 µt']
['(in conditional form) by']
['µh(ah|xh) :=']
['1:h (xh, ah)']
['1:h−1 (xh)']
[', (16) for any h and (xh, ah) ∈ Xh × A. It is straightforward to check that this µ is exactly the averaging of µt in the sequence-form representation (see e.g. (Kozuno et al., 2021, Theorem 1)):']
['µ1:h(xh, ah) =']
['µt']
['1:h(xh, ah) for all (h, xh, ah). (17)']
['Both expressions above can be used as the definition interchangably. The average policy of the min-player']
['ν = 1']
['PT t=1 νt is defined similarly.']
['C.2 Balanced exploration policy']
['Lemma C.4 (Balancing property of µ⋆,h']
['). For any max-player’s policy µ ∈ Πmax and any h ∈ [H], we have']
['µ1:h(xh, ah)']
['µ⋆,h']
['1:h(xh, ah)']
['= XhA.']
['Lemma C.4 states that µ⋆,h is a good exploration policy in the sense that the distribution mismatch between it and any µ ∈ Πmax has bounded L1 norm. Further, the bound XhA is non-trivial—For example, if we replace µ⋆,h']
['1:h with the uniform policy µunif']
['1:h (xh, ah) = 1/Ah']
[', the left-hand side can be as large as XhAh in the worst case.']
['Proof of Lemma C.4 We have']
['X xh,ah']
['X xh−1,ah−1']
['(xh,ah)∈C(xh−1,ah−1)×A']
['µ1:(h−1)(xh−1, ah−1) · µh(ah|xh)']
['1:(h−1)(xh−1, ah−1) · (1/A)']
['(i)']
['= A ·']
['X xh∈C(xh−1,ah−1)']
['µ1:(h−1)(xh−1, ah−1)']
['1:(h−1)(xh−1, ah−1)']
['· |Ch(xh−1, ah−1)|']
['(ii)']
['X xh−2,ah−2']
['(xh−1,ah−1)∈C(xh−2,ah−2)×A']
['µ1:(h−2)(xh−2, ah−2)µh−1(ah−1|xh−1)']
['1:(h−2)(xh−2, ah−2) · |Ch(xh−1, ah−1)|/|Ch(xh−1)|']
['1:(h−2)(xh−2, ah−2)']
['· |Ch(xh−1)|']
['(iii)']
['µ1:(h−2)(xh−2, ah−2)']
['· |Ch(xh−2, ah−2)|']
['= . . .']
['X x1,a1']
['µ1(a1|x1)']
['|Ch(x1, a1)|/|Ch(x1)|']
['· |Ch(x1, a1)|']
['µ1(a1|x1) · |Ch(x1)|']
['X x1']
['|Ch(x1)| = A · |Ch(∅)| = XhA.']
['Above, (i) used the definition of µ⋆,h h and the fact that']
['P ah∈A µh(ah|xh) = 1 for any µ, xh; (ii) used the definition of µ⋆,h h−1; (iii) used the fact that']
['P xh−1∈C(xh−2,ah−2) |Ch(xh−1)| = |Ch(xh−2, ah−2)| which follows by the additivity of the number of descendants; and the rest followed by performing the same operations repeatedly.']
['The following corollary is similar to the lower bound in (Farina et al., 2020c, Appendix A.3).']
['Corollary C.5. We have']
['1:h(xh, ah) ≥']
['XhA for any h ∈ [H] and (xh, ah) ∈ Xh × A.']
['Proof. Choose some deterministic policy µ s.t. µ1:h(xh, ah) = 1 in Lemma C.4 and noticing each term in the summation is non-negative, µ1:h(xh, ah)']
['≤ XhA.']
['C.2.1 Interpretation as a transition probability']
['We now provide an intepretation of the balanced exploration policy µ⋆,h']
['1:h: its inverse 1/µ⋆,h']
['1:h can be viewed as the (product) of a “transition probability” over the game tree for the max player. As a consequence, this interpretation also provides an alternative proof of Lemma C.4.']
['For any 1 ≤ h ≤ H and 1 ≤ k ≤ h − 1, denote p⋆,h k (xk+1|xk, ak) = |Ch(xk+1)|/|Ch(xk, ak)| (we use the convention that |Ch(xh)| = 1). By this definition, p⋆,h k (·|xk, ak) is a probability distribution over Ch(xk, ak) and can be interpreted as a balanced transition probability from (xk, ak) to xk+1. We further denote the sequence form of the balanced transition probability by p⋆,h']
['|Ch(x1)|']
['Xh h−1']
['Y k=1 p⋆,h k (xk+1|xk, ak) =']
['Y k=1']
['|Ch(xk+1)|']
['|Ch(xk, ak)|']
['. (18)']
['Lemma C.6. For any (xh, ah) ∈ Xh × A, the sequence form of the transition p⋆,h']
['1:h(xh) and the sequence form of balanced exploration strategy µ⋆,h']
['1:h(xh, ah) are related by p⋆,h']
['XhA · µ⋆,h']
['. (19)']
['Furthermore, for any max player’s policy µ ∈ Πmax and any h ∈ [H], we have']
['µ1:h(xh, ah)p⋆,h']
['1:h(xh) = 1. (20)']
['Proof of Lemma C.6 By the definition of the balanced transition probability as in Eq. (18) and the balanced exploration strategy as in Eq. (5), we have']
['XhA h−1']
['|Ch(xk)|']
['× A =']
['= p⋆,h']
['1:h(xh). where the second equality used the property that |Ch(xh)| = 1. This proves Eq. (19). The proof of Eq. (20) is similar to the proof of Lemma C.1 (a).']
['Alternative proof of Lemma C.4 Lemma C.4 follows as a direct consequence of Eq. (19) and (20) in']
['Lemma C.6.']
['C.3 Balanced dilated KL']
['Lemma C.7 (Bound on balanced dilated KL). Let µunif']
['∈ Πmax denote the uniform policy: µunif h (ah|xh) =']
['1/A for all (h, xh, ah). Then we have max']
['µ†∈Πmax']
['Dbal']
['(µ† kµunif']
[') ≤ XA log A.']
['Proof. We have max']
[') = max']
['X h=1']
['µ†']
['1:h(xh, ah) log']
['µ† h(ah|xh)']
['µunif h (ah|xh)']
['= max']
['\x10 log µ† h(ah|xh) + log A']
['≤ log A']
['X h=1 max']
['= log A']
['XhA = XA log A, where (i) is because µ† h(ah|xh) log µ† h(ah|xh) ≤ 0 (recalling that each sequence form µ†']
['1:h(xh, ah) contains the term µ† h(ah|xh)), and (ii) uses the balancing property of µ⋆,h']
['(Lemma C.4).']
['C.3.1 Interpretation of balanced dilated KL']
['We present an interpretation of the balanced dilated KL (6) as a KL distance between the reaching probabilities under the “balanced transition” (18) on the max player’s game tree.']
['For any policy µ ∈ Πmax, we define its balanced transition reaching probability Pµ,⋆ h (xh, ah) as']
['Pµ,⋆ h (xh, ah) = µ1:h(xh, ah)p⋆,h']
['1:h(xh). (21)']
['This is a probability measure on Xh × A ensured by Lemma C.6. For any two probability distribution p and q, we denote KL(pkq) to be their KL divergence.']
['Lemma C.8. For any tuple of max-player’s policies µ, ν ∈ Πmax, we have']
['(µkν) =']
['(XhA)KL(Pµ1:h,⋆ h kP']
['µ1:h−1νh,⋆ h ). (22)']
['Proof of Lemma C.8 By Eq. (21) and by the definition of KL divergence, we have']
['(XhA)Dkl']
['(Pµ1:h,⋆ h kP']
['µ1:h−1νh,⋆ h )']
['= (XhA)']
['1:h(xh) log h µ1:h(xh, ah)p⋆,h']
['1:h(xh)']
['µ1:h−1(xh−1, ah−1)νh(xh|ah)p⋆,h']
['1:h(xh) i']
['1:h(xh, ah) log hµh(ah|xh)']
['νh(ah|xh) i']
[', (23) where the last equality is by Lemma C.6. Comparing with the definition of Dbal as in Eq. (6) concludes the proof.']
['D Proofs for Section 3']
['D.1 Efficient implementation for Update (9)']
['Algorithm 5 Implementation of Balanced OMD update']
['Require: Current policy µt']
['; Trajectory (xt']
['1, at']
['1, . . . , xt']
['H, at']
['H); learning rate η > 0;']
['Loss vector n e']
['ℓt h(xh, ah) o h,xh,ah that is non-zero only on (xh, ah) = (xt h, at h).']
['1: Set Zt']
['H+1 ← 1.']
['2: for h = H, . . . , 1 do']
['3: Compute normalization constant']
['Zt h ← 1 − µt h(at h|xt h) + µt h(at h|xt h) · exp −ηµ⋆,h']
['1:h(xt h, at h)e']
['ℓt h(xt h, at h) +']
['1:h(xt h, at h) log Zt h+1']
['µ⋆,h+1']
['1:h+1(xt h+1, at h+1)']
['!']
['4: Update policy at xt h:']
['µt+1 h (ah|xt h) ←']
['\uf8f1']
['\uf8f4']
['\uf8f2']
['\uf8f3']
['µt h(ah|xt h) · exp −ηµ⋆,h']
['− log Zt h']
['! if ah = at h, µt h(ah|xt h) · exp(− log Zt h) otherwise.']
['5: Set µt+1 h (·|xh) ← µt h(·|xh) for all xh ∈ Xh \\']
['\x08 xt h .']
['Ensure: Updated policy µt+1']
['Lemma D.1. Algorithm 5 indeed solves the optimization problem (9):']
['µt+1']
['← arg min']
['µ∈Πmax']
['D']
['µ, e']
['(µkµt']
[').']
['Proof. First, by the sparsity of the loss estimator e']
['(cf. (8)), the above objective can be written succinctly as']
[') (24)']
['" e']
['ℓt h (xh, ah) +']
['ηµ⋆,h']
['µh(ah|xh)']
['µt h(ah|xh)']
['X xh']
['µ1:h−1(xh)']
['µh(·|xh), e']
['ℓt h (xh, ·)']
['KL (µh(·|xh)||µt h(·|xh))']
['µ1:h−1(xt h)']
['µh(at h|xt h)e']
['ℓt h xt h, at h']
['\x01']
['1:h(xt h, ah)']
['X xh6=xt h']
['\uf8fc']
['\uf8fd']
['\uf8fe']
['(25)']
['We now show the equivalence by backward induction over h = H, . . . , 1. For h = H, we can optimize over the H-th layer directly to see']
['H (aH|xt']
['H) ∝aH µt']
['H(aH|xt']
['H) exp n']
['−ηµ⋆,h']
['1:h(xt h, ah)e']
['H(xt']
['H, aH) o']
['= µt']
['H ) exp n']
['H , aH) − log Zt']
['H o']
[', where Zt']
['H > 0 is the normalization constant. For all non-visited xH 6= xt']
['H , by equation (25) and nonnegativity of KL divergence, the object must be minimized at µt+1']
['H (·|xH ) = µt h(·|xH).']
['If the claim holds from layer h + 1 to H, consider the h-th layer. Plug in the proved optimizer after layer h, the objective (25) can be written as']
['X h′=1']
['X xh′ ,ah′']
['µ1:h′ (xh′ , ah′ )']
['ℓt h′ (xh′ , ah′ ) +']
['ηµ⋆,h′']
['1:h′ (xh′ , ah′ ) log']
['µh′ (ah′ |xh′ )']
['µt h′ (ah′ |xh′ )']
['X xh′']
['µ1:h′−1(xh′ )']
['µh′ (·|xh′ ), e']
['ℓt h′ (xh′ , ·)']
['KL (µh′ (·|xh′ )||µt h′ (·|xh′ ))']
['1:h′ (xh′ , ah′ )']
['= h']
['X h′=h+1']
['µ1:h′ (xt h′ , at h′ ) log Zt h′+1']
['ηµ⋆,h′+1']
['1:h′+1(xt h′+1, at h′+1)']
['µ1:h′−1(xt h′−1, at h′−1) log Zt h′']
['1:h′ (xt h′ , at h′ )']
['µ1:h(xt h, at h) log Zt h+1']
['ηµ⋆,h+1']
['= h−1']
['+ µ1:h−1(xt h)']
['µh(at h|xt h)']
['\x10 e']
['− log Zt h+1']
['KL (µh(·|xt h)||µt h(·|xt h))']
['Thus in the h layer we can optimize by setting']
['µt+1 h (ah|xt h) = µt h(ah|xt h) exp']
['(']
['ℓt h(xt h, ah) −']
['1:h+1(xt h+1, at h+1) log Zt h+1']
['\x08 ah = at h − log Zt h']
[')']
['For all non-visited xh 6= xt h, by non-negativity of KL divergence, the object must be minimized at µt+1 h (·|xh) =']
['µt h(·|xh). This is exactly the update rule in Algorithm 5.']
['D.2 Proof of Theorem 4']
['Decompose the regret as']
['RT']
['− µ†']
[', ℓt']
['(26)']
['| {z }']
['BIAS1']
['+ max']
['− ℓt']
['BIAS2']
['REGRET']
['. (27)']
['We now state three lemmas that bound each of the three terms above. Their proofs are presented in']
['Section D.4, D.5, and D.6 respectively. Below, ι := log(3HXA/δ) denotes a log factor.']
['Lemma D.2 (Bound on BIAS1']
['). With probability at least 1 − δ/3, we have']
['≤ H']
['√']
['2T ι + γHT.']
['Lemma D.3 (Bound on BIAS2']
['≤ XAι/γ.']
['Lemma D.4 (Bound on REGRET). With probability at least 1 − δ/3, we have']
['REGRET ≤']
['XA log A']
['+ ηH3']
['T +']
['ηH2']
['XAι']
['γ']
['Putting the bounds together, we have that with probability at least 1 − δ, RT']
['+ H']
['2T ι + γHT +']
['Set η = q']
['H3T and γ = q']
['T H , we have']
['≤ 6']
['XAH3T ι + HXAι.']
['Additionally, recall the naive bound RT']
['≤ HT on the regret (which follows as hµt']
[', ℓt i ∈ [0, H] for any']
['µ ∈ Πmax, t ∈ [T ]), we get']
['≤ min n']
['6']
['XAH3T ι + HXAι, HT o']
['≤ HT · min n']
['6 p']
['XAHι/T + XAι/T, 1 o']
['For T > HXAι, the min above is upper bounded by 7 p']
['HXAι/T. For T ≤ HXAι, the min above is upper bounded by 1 ≤ 7 p']
['HXAι/T. Therefore, we always have']
['≤ HT · 7 p']
['HXAι/T = 7']
['H3XAT ι.']
['This is the desired result.']
['The rest of this section is devoted to proving the above three lemmas.']
['D.3 A concentration result']
['We begin by presenting a useful concentration result. This result is a variant of (Kozuno et al., 2021, Lemma']
['3) and (Neu, 2015, Lemma 1) suitable to our loss estimator (8) where the IX bonus on the denominator depends on (xh, ah).']
['Lemma D.5. For some fixed h ∈ [H], let αt h (xh, ah) ∈ h']
['0, 2γµ⋆,h']
['1:h (xh, ah) i be Ft−1']
['-measurable random variable for each (xh, ah) ∈ Xh × A. Then with probability 1 − δ, T']
['αt h (xh, ah)']
['ℓt h (xh, ah) − ℓt h (xh, ah)']
['≤ log (1/δ) .']
['Proof. Define the unbiased importance sampling estimator b']
['ℓt h :=']
['1 − rt h']
['1:h(xt h, at h)']
['· 1']
['\x08 xh = xt h, ah = at h .']
['We first have e']
['ℓt h (xh, ah) =']
['1:h(xh, ah) + γµ⋆,h']
['\x08 xh = xt h, ah = at h']
['1:h(xh, ah) (1 − rt h)']
['2γµ⋆,h']
['1:h(xh, ah) (1 − rt h) 1 {xh = xt h, ah = at h} /µt']
['1 + γµ⋆,h']
['1:h(xh, ah)b']
['ℓt h(xh, ah)']
['1 + 2γµ⋆,h']
[', where (i) is because for any z ≥ 0, z']
['1+z/2 ≤ log (1 + z).']
['As a result, we have the following bound on the moment generating function:']
['( exp']
['αt h (xh, ah) e']
['ℓt h (xh, ah)']
['|Ft−1']
['≤E']
['X xh,ah log']
['1 + αt h (xh, ah) b']
['=E']
['Y xh,ah']
['= E']
['1 +']
['αt h (xh, ah) b']
['ℓt h(xh, ah)|Ft−1']
['=1 +']
['αt h (xh, ah) ℓt h(xh, ah)']
[', where (i) is because z log (1 + z′']
[') ≤ log (1 + zz′']
[') for any 0 ≤ z ≤ 1 and z′']
['> −1, and (ii) follows from the fact that for any h, at most one of b']
['ℓt h(xh, ah) is non-zero, so the cross terms disappear.']
['Repeating the above argument, E']
['( T']
['))']
['(T −1']
['αT h (xh, ah)']
['ℓT h (xh, ah) − ℓT h (xh, ah)']
['|FT −1']
['≤ · · · ≤ 1.']
['Therefore, we can apply the Markov inequality and get']
['P']
['> log (1/δ)']
['=P']
['> 1/δ']
['≤δ · E']
['≤ δ.']
['Corollary D.6. We have']
['(a) For some fixed h ∈ [H] and (xh, ah), let αt h (xh, ah) ∈ h']
['-measurable random variable. Then with probability 1 − δ, T']
['(b) For some fixed h ∈ [H] and xh, let αt h (xh, ah) ∈ h']
['-measurable random variable for each ah ∈ A. Then with probability 1 − δ, T']
['X ah∈A']
['Proof. For (a), using Lemma D.5 with (αt h)']
['′']
['(x′ h, a′ h) = αt h (x′ h, a′ h) 1 {x′ h = xh, a′ h = ah}, T']
['X x′ h']
[',a′ h']
['αt h (xh, ah) 1 {x′ h = xh, a′ h = ah} h e']
['(x′ h, a′ h) − ℓt']
['(x′ h, a′ h) i']
['Claim (b) can proved similarly.']
['D.4 Proof of Lemma D.2']
['We further decompose BIAS1 to two terms by']
['− E n e']
['|Ft−1 oE']
['(A)']
[', E n e']
['|Ft−1 o']
['(B)']
['To bound (A), plug in the definition of loss estimator, T']
['ℓt h(xh, ah) −']
['1:h(xh, ah)ℓt h(xh, ah)']
['γµ⋆,h']
['γ =γHT, where (i) is by using Corollary C.2 with policy µ⋆,h for each layer h.']
['To bound (B), first notice']
['(1 − rt h) 1 {xh = xt h, ah = at h}']
['\x08 xh = xt h, ah = at h =']
['1 = H.']
['Then by Azuma-Hoeffding, with probability at least 1 − δ/3, T']
['≤ H p']
['2T log(3/δ) ≤ H']
['2T ι.']
['Combining the bounds for (A) and (B) gives the desired result.']
['D.5 Proof of Lemma D.3']
['We have']
['1:h(xh, ah) h e']
['ℓt h(xh, ah) − ℓt h(xh, ah) i']
['≤ log (XA/δ)']
['ι']
['XhA = XAι/γ, where (i) is by applying Corollary D.6 for each (xh, ah) pair and taking union bound, and (ii) is by']
['Lemma C.4.']
['D.6 Proof of Lemma D.4']
['We begin by stating the following lemma, which roughly speaking relates the task of bounding the regret to bounding the term']
['+ 1']
['ηµ⋆,1']
['1:1 (xt']
['1,a1) log Zt']
['1.']
['Lemma D.7. For any policy µ ∈ Πmax, Dbal']
['(µkµt+1']
[') − Dbal']
[') = η']
['µ⋆,1']
['1:1(xt']
['1, a1) log Zt']
['Proof. By definition of Dbal and the conditional form update rule in Algorithm 1, Dbal']
['µt+1 h (ah|xh)']
['X ah']
['µ1:h(xt h, ah)']
['1:h(xt h, ah) log']
['µt h(ah|xt h)']
['µt+1 h (ah|xt h)']
['µ1:h(xt h, at h)']
['ℓt h −']
['1:h(xt h, ah) log Zt h']
['=η']
['µ1:h(xt h, at h)e']
['ℓt h(xt h, at h) −']
['1:h+1(xt h+1, at h+1) log Zt h+1 +']
['µ1:h−1(xt h−1, at h−1)']
['1:h(xt h, at h) log Zt h']
['Additional notation We introduce the following notation for convenience throughout the rest of this subsection. Define']
['βt h := ηµ⋆,h']
['1:h(xt h, at h).']
['For simplicity, when there is no confusion, we write']
['µt h := µt h(at h|xt h), µt h:h′ := h′']
['Y h′′=h']
['µt h′′ , and e']
['ℓt h := e']
['1:h(xt h, at h) + γµ⋆']
['Define the normalized log-partition function as']
['Ξt h :=']
['βt h log Zt h =']
['βt h log']
['1 − µt h + µt h exp h']
['βt h']
['Ξt h+1 − e']
['ℓt h']
['\x11i\x11']
['Note that this value can be seen as an H-variate function of the loss estimator n e']
['ℓt h o h∈[H]']
['. To make this dependence more clear, for any e']
['ℓ ∈ [0, ∞)H']
[', we define the function {Ξt h (·)}']
['H h=1 recursively by (overloading notation)']
['Ξt h']
['ℓ']
['= Ξt h']
['ℓh:H']
[':=']
['\uf8f3 log']
['−βt h e']
['ℓh i\x11']
['/βt h if h = H, log']
['Ξh+1']
['ℓh+1:H']
['ℓh']
['/βt h otherwise.']
['With this definition, we have Ξt h = Ξt h']
['\x11 where e']
['ℓt is the actual loss estimator. Note that, importantly, Ξt h(e']
['ℓh:H) has a compositional structure: It is a function of e']
['ℓh (h-th entry of the loss) and Ξt h+1 (which is itself a function of e']
['ℓh+1:H). This compositional structure is key to proving bounds on its gradients and Hessians.']
['The rest of this subsection is organized as follows. In Section D.6.1, we bound the gradients and Hessians of the function Ξt']
['1(·) in an entry-wise fashion, and then use the Mean-Value Theorem to give a bound on']
['Ξt']
['1 = Ξt']
['1(e']
[') (Lemma D.11). We then combine this result with Lemma D.7 to prove the main lemma that bounds REGRET (Section D.6.2).']
['D.6.1 Bounding Ξt']
['Lemma D.8. For e']
['ℓ ∈ [0, ∞)H and any h ∈ [H], Ξt h']
['≤ 0. Furthermore, Ξt h (0) = 0.']
['Proof. We show the first claim by backward induction. For h = H, Ξt']
['ℓH']
['= log']
['1 − µt']
['H + µt']
['H exp h']
['−βt']
['H e']
['ℓH i\x11']
['/βt']
['H ≤ log 1 − µt']
['H ≤ 0, because e']
['H ≥ 0.']
['Assume Ξt h+1']
['≤ 0, then for the previous step h, Ξt h']
['Ξt h+1']
['/βt h ≤ log 1 − µt h + µt h']
['/βt h ≤ 0.']
['The second claim follows as all inequalities become equalities at e']
['ℓ = 0.']
['Lemma D.9 (Bounds on first derivatives). For e']
['ℓ ∈ [0, 1]H and any h ∈ [H], the derivatives are bounded by']
['0 ≤']
['∂Ξt h']
['∂Ξt h+1']
['≤ µt h and − µt h ≤']
['∂e']
['≤ 0.']
['Furthermore, ∂Ξt h']
['ℓh′ e']
['ℓ=0']
['− µt h:h′ if h′']
['≥ h, 0 otherwise.']
['Proof. By chain rule and the compositional structure of the functions Ξt h(·), for any h′']
['≥ h, ∂Ξt h']
['ℓh′']
['∂Ξt h′']
['·']
['\uf8ed h′']
['−1']
['∂Ξt h′′']
['∂Ξt h′′+1']
['\uf8f8 ·']
['For any h, the derivatives are bounded by']
['µt h exp h']
['\x11i']
['\x11i ∈']
['0, µt h']
[', ∂Ξt h']
['= −']
['−µt h, 0']
['The inequalities hold because the function f (z) =']
['µt hz']
['1−µt h+µt hz']
['= 1 −']
['1−µt h']
['1−µt h+µt hz is increasing on z ∈ [0, 1], and exp h']
['∈ [0, 1] by Lemma D.8.']
['Putting them together, at e']
['ℓ = 0, the derivative is just']
['ℓt=0']
['= −µt h:h′ if h′']
['≥ h. If h′']
['< h, since Ξt h only depends on loss in the later layers, ∂Ξt h']
['|e']
['ℓt=0 = 0.']
['Lemma D.10 (Bounds on second derivatives). For e']
['ℓ ∈ [0, 1]H and any h ∈ [H], if h′']
['≥ h and h′′']
['≥ h, the second-order derivatives are bounded by']
['∂2']
['ℓh′ ∂e']
['ℓh′′']
['≤ min{h′']
[',h′′']
['}']
['X h′′′=h']
['βt h′′′ µt h:h′ µt h′′′+1:h′′ = min{h′']
['βt h′′′ µt h:h′′′ µt h′′′+1:h′ µt h′′′+1:h′′ .']
['Otherwise']
['= 0.']
['Proof. By symmetry of the second derivatives and the right-hand side with respect to h′ and h′′']
[', it suffices to prove the claim for h′′']
['≥ h′ only.']
['By chain rule and the compositional structure of the functions Ξt h(·), ∂2']
['∂Ξt h′ ∂e']
['Ξt h′']
['If h′′']
['= h′']
['= h, ∂2']
['ℓ2 h']
['= βt hµt h exp h']
['\x11i 1 − µt h n']
['\x11io2 ≤ βt hµt h.']
['If h′']
['= h, h′′']
['> h, ∂2']
['ℓh∂e']
['(1 − µt h)βt hµt h exp h']
['\x11i\x112 ·']
['≤ βt hµt h:h′′ .']
['If h < h′']
['< h′′']
[', we can compute the Hessian by induction. Notice once h′']
['> h we have']
['Take second derivative, ∂2']
['∂Ξt h+1∂e']
['We first bound the second term, (ii) =']
['≤ βt hµt h · µt h+1:h′′ · µt h+1:h′']
['≤ βt hµt h:h′ µt h+1:h′′ .']
['The first term can be simplified to']
['(i) ≤']
['≤ µt h']
['Now plug in']
['≤ βt h′ µt h′:h′′ and backward induction from h′ to h gives:']
['≤ h′']
['βt h′′′ µt h:h′ µt h′′′+1:h′′ .']
['We can check this expression is also correct for the above special cases when h′']
['= h. The second claim holds because Ξt h only depends on loss in the later layers.']
['Lemma D.11 (Bound on Ξt']
['1). We have']
['1 ≤ −']
['ηH']
['X h′=h']
['1:h (xh′ , ah′ ) µt h+1:h′ (xh′ , ah′ ) e']
['ℓt h′ (xh′ , ah′ )']
['\uf8f8.']
['Proof. We apply the Mean-value Theorem to function Ξt']
['\x11 at e']
['ℓ = 0, Ξt']
['= Ξt']
['1 (0) +']
['∇e']
['ℓΞt']
['1 e']
['∇2 e']
['ℓ=ξt e']
[', where ξt lies on the line segment between 0 and e']
['By Lemma D.8, the initial term is just zero. By Lemma D.9, the first-order term is just −']
['It thus remains to bound the second-order term. Applying the entry-wise upper bounds in Lemma D.10 at h = 1 (which hold uniformly at all nonnegative loss values, including ξt']
['), we have']
['ℓt h e']
['ℓt h′']
['X h′=1 min{h,h′']
['X h′′=1']
['βt h′′ µt']
['1:hµt h′′+1:h′ e']
['1:h e']
['βt h′′ µt h′′+1:h′ e']
['≤ H max h∈[H]']
['=H']
['X h′=1 h′']
['X h′=h′′']
['=ηH']
['µ⋆,h′′']
['1:h′′ xt h′ , at h′']
['µt h′′+1:h′ e']
['= ηH']
['1:h′′ (xh′ , ah′ ) µt h′′+1:h′ (xh′ , ah′ ) e']
['\uf8f8, where (i) is by Lemma D.10; (ii) follows from the bound']
['ℓt h =']
['1:h ·']
['1:h + γµ⋆,h']
['1:h']
['≤ H; and (iii) is because e']
['ℓt h′ (xh′ , ah′ ) = 0 at all (xh′ , ah′ ) 6= (xt h′ , at h′ ).']
['Lemma D.12. With probability at least 1 − δ/3, T']
['ηXAH2']
[', where ι := log(H/δ).']
['Proof. Using Lemma D.11 and take the summation with respect to t ∈ [T ] we have']
['X xh′,ah′']
['1:h (xh′, ah′) µt h+1:h′ (xh′, ah′) e']
['ℓt h′ (xh′, ah′)']
[':=∆t h,h′']
['. (28)']
['Observe that the random variables ∆t h,h′ satisfy the following:']
['• ∆t h,h′ ≤ Xh′ A/γ almost surely:']
['∆t h,h′ =']
['1:h (xh′ , ah′ ) µt h+1:h′ (xh′, ah′)']
['(1 − rt h′ ) 1 {xh′ = xt h′ , ah′ = at h′ }']
['1:h′ (xh′, ah′) + γµ⋆,h′']
['1:h′ (xh′, ah′)']
['µ⋆,h′']
['Xh′ A']
[', where (i) is by using Lemma C.4 with the mixture of µ⋆,h and µt']
['• E[∆t h,h′|Ft−1] ≤ 1, where Ft−1 is the σ-algebra containing all information after iteration t − 1:']
['E[∆t h,h′|Ft−1] =']
['1:h (xh′ , ah′ ) µt h+1:h′ (xh′, ah′) ℓt h′ (xh′, ah′)']
['≤ 1, where (i) is by using Corollary C.2 with the mixture policy of µ⋆,h and µt']
['• The conditional variance E[(∆t h,h′)2']
['|Ft−1] can be bounded as']
['E[(∆t h,h′)2']
['|Ft−1]']
['\uf8ee']
['\uf8f0 µ⋆,h']
['!2']
['\uf8f9']
['\uf8fb']
[', where (i) follows from the fact that for any h, at most one of indicators is non-zero, so the cross terms disappear and (ii) is using Corollary C.2 with the mixture policy of µ⋆,h and µt']
['Therefore, we can apply Freedman’s inequality (Lemma A.1) and union bound to get that, with probability at least 1 − δ/3, for some fixed λh,h′ ∈ (0, γ/Xh′ A], the following holds simultaneously for all h, h′']
[':']
['∆t h,h′ ≤']
['λh,h′Xh′ AT']
['2 log(H/δ)']
['λh,h′']
['+ T, Take λh,h′ = γ/Xh′ A, we have']
['Xh′ A · 2 log(H/δ)']
['+ 2T.']
['Plug into equation (28), we have']
[', where ι := log(H/δ) is a log factor.']
['D.6.2 Proof of main lemma']
['By Lemma D.7, for any policy µ†']
['∈ Πmax, 1']
['(µ† kµt+1']
['(µ† kµt']
['+ Ξt']
['Taking the summation w.r.t. t ∈ [T ] and using Lemma D.12, we have with probability at least 1 − δ/3, the following holds simultaneously over all µ†']
['∈ Πmax:']
['(µ† kµT']
['(µ† kµ1']
[') =']
['− µt']
['Rerranging the terms we have max']
['≤ max']
[') + ηH3']
[', where the last inequality above follows by recalling that µ1 is taken to be the uniform policy (µ1 h(ah|xh) = 1/A for all (h, xh, ah)) in Algorithm 1, and applying the bound on the balanced dilated KL (Lemma C.7). This proves Lemma D.4.']
['D.7 Proof of Theorem 6']
['Both the regret and PAC lower bounds follow from a direct reduction to stochastic multi-armed bandits.']
['For completeness, we first state the lower bound for stochastic bandits (Lattimore and Szepesvári, 2020, Exercise 15.4 & Exercise 33.1) as follows. Below, c is an absolute constant.']
['Proposition D.13 (Lower bound for stochastic bandits). Let K ≥ 2 denote the number of arms.']
['(a) (Regret lower bound) Suppose T ≥ K. For any bandit algorithm that plays policy µt']
['∈ ∆([K]) (either deterministic or random) in round t ∈ [T ], there exists some K-armed stochastic bandit problem with']
['Bernoulli rewards with mean vector r ∈ [0, 1]K']
[', on which the algorithm suffers from the following lower bound on the expected regret:']
['" max']
['µ†∈∆([K])']
[', r']
['≥ c ·']
['KT.']
['(b) (PAC lower bound) For any bandit algorithm that plays for t rounds and outputs some policy b']
['µ ∈']
['∆([K]), there exists some K-armed stochastic bandit problem with Bernoulli rewards with some mean vector r ∈ [0, 1]K']
[', on which policy b']
['µ is at least ε away from optimal:']
['\x14 max']
['− b']
['µ, r']
['\x15']
['≥ ε, unless T ≥ cK/ε2']
['We now construct a class of IIEFGs with XH = AH−1']
['(the minimal possible number of infosets), and show that any algorithm that solves this class of games will imply an algorithm for stochastic bandits with AH arms with the same regret/PAC bounds, from which Theorem 6 follows.']
['Our construction is as follows: For any A ≥ 2 and H ≥ 1, we let Sh = Ah−1 for all h ∈ [H] (in particular, S1 = 1) and B = 1 (so that there is no opponent effectively). By the tree structure, each state is thus uniquely determined by all past actions sh = (a1, . . . , ah−1), and the transition is deterministic: ((a1, . . . , ah−1), ah) ∈']
['Sh × A transits to (a1, . . . , ah) ∈ Sh+1 with probability one. Further, we let xh = x(sh) = sh, so that there is no partial observability, and thus Xh = Sh for all h. Only the H-th layer yields a Bernoulli reward with some mean ra1:H']
[':= E[rH (a1:H−1, aH)] ∈ [0, 1], for all a1:H ∈ XH. The reward is zero within all previous layers.']
['Under this model, the expected reward under any policy µ ∈ Πmax can be succinctly written as hµ, ri =']
['(xH ,aH )∈XH×A']
['µ1:H(xH , aH)E[rH (xh, aH)] =']
['X a1:H ∈AH']
['µ1:H(a1:H)ra1:H .']
['This expression coincides with the expression for the expected reward of an AH']
['-armed stochastic bandit problem.']
['Now, for any algorithm Alg achieving regret RT on IIEFGs, we claim we can use it to design an algorithm for solving any AH']
['-armed stochastic bandit problem with Bernoulli rewards, and achieve the same regret.']
['Indeed, given any AH']
['-armed bandit problem, we rename its arms as a sequence a1:H = (a1, . . . , aH) ∈ AH']
['Now, we instantiate an instance of Alg on a simulated IIEFG with the above structure. Whenever Alg plays policy µt']
['∈ Πmax, we query an arm a1:H using policy µt']
['1:H(·) ∈ ∆(AH']
[') in the bandit problem. Then, upon receiving the reward rt from the bandit problem, we give the feedback that the game transitted to infoset a1:H and yielded reward rt']
['. By the above equivalence, the regret RT within this simulated game is exactly the same as the regret for the bandit problem.']
['Therefore, for T ≥ AH']
[', we can apply Proposition D.13(a) to show that for any such Alg, there exists one such IIEFG, on which']
['AHT = c p']
['XHAT ≥ c']
['XAT, where the last inequality follows from the fact that X ≤ XH(1 + 1/A + 1/A2']
['+ · · · ) ≤ XH/(1 − 1/A) ≤ 2XH by perfect recall. This shows part (a).']
['Part (b) (PAC lower bound) follows similarly from Proposition D.13(b). Using the same reduction, we can show for any algorithm that controls both players and outputs policy (b']
['µ, b']
['ν) ∈ Πmax × Πmin, there exists one such game of the above form (where only the max player affects the game) where the algorithm suffers from the PAC lower bound']
['E[NEGap(b']
['ν)] = E']
['V µ†']
[',b']
['ν']
['− V b']
['µ,b']
['≥ ε unless T ≥ cXA/ε2']
['. The symmetrical construction for the min player implies that there exists some game on which E[NEGap(b']
['ν)] ≥ ε unless T ≥ cY B/ε2']
['Therefore, if T < c(XA + Y B)/(2ε2']
['), at least one of T ≥ cXA/ε2 and T ≥ cY B/ε2 has to be false, for which we obtain a game where the expected duality gap is at least ε. This shows part (b).']
['E Proofs for Section 4']
['E.1 Counterfactual regret decomposition']
['Define the immediate counterfactual regret at any xh ∈ Xh, h ∈ [H] as']
['Rimm,T h (xh) = max']
['µ† h(·|xh)']
['µt h(·|xh) − µ† h(·|xh), Lt h(xh, ·)']
[', (29) where Lt h(·, ·) is the counterfactual loss function defined in (10):']
['Lt h(xh, ah) := ℓt h(xh, ah) +']
['(xh′ ,ah′ )∈Ch′ (xh,ah)×A']
['(h+1):h′ (xh′ , ah′ )ℓt h′ (xh′ , ah′ ).']
['Lemma E.1 (Counterfactual regret decomposition). We have e']
['PH h=1 RT h , where']
['RT h :=']
['X x1∈X1 max a1∈A']
['· · ·']
['X xh−1∈C(xh−2,ah−2) max ah−1∈A']
['Rimm,T h (xh), = max']
['X xh∈Xh']
['µ1:(h−1)(xh−1, ah−1) · Rimm,T h (xh).']
['Proof. The bound e']
['PH h=1 RT h with the sum-max form expression for e']
['RT h has already implicitly appeared in the proof of (Zinkevich et al., 2007, Theorem 3), albeit with their slightly different formulation of extensiveform games (turn-based games with reward only in the last round). For completeness, here we provide a proof under our formulation.']
['We first show the bound with the µ form expression for RT h , which basically follows by a performance decomposition argument. We have e']
['1:h−1µt h:H − µ†']
['1:hµt h+1:H, ℓt']
[':=RT h']
['Note that each term RT h measures the performance difference between µ†']
['1:h−1µt h:H and µ†']
['1:hµt h+1:H:']
['RT h = max']
['Esh∼µ†']
['1:h−1×νt']
['Eah∼µt(·|xh)']
['X h′=1 rh′']
['− Eah∼µ†(·|xh)']
['##']
['1:h−1(xh−1, ah−1) ·']
['1:h−1(xh−1, ah−1) · Rimm,T h (xh).']
['Above, (i) follows as the rewards for the first h−1 steps are the same for the two expectations; (ii) follows by definition of the counterfactual loss function (cumulative loss multiplied by the opponent and environment’s policy / transition probabilities, as well as the max player’s own policy from step h onward). The claim']
['(with the µ form expression) thus follows by renaming the dummy variable µ† as µ.']
['To verify that the second expression is equivalent to the first expression, it suffices to notice that the max over µ1:h−1 ∈ Πmax consists of separable optimization problems over µh′ (·|xh′ ) over all xh′ ∈ Xh′ , h′']
['≤ h− 1, due to the perfect recall assumption (different (xh′ , ah′ ) leads to disjoint subtrees). Therefore, we can rewrite the above as']
['RT h =']
['X x1∈X1 max']
['µ1(·|x1)∈∆(A)']
['X a1∈A']
['X x2∈C(x1,a1)']
['X xh−1∈C(xh−2,ah−2) max']
['µh−1(·|xh−1)∈∆(A)']
['X ah−1∈A']
['µh−1(ah−1|xh−1)']
['Rimm,T h (xh).']
['Further noticing (backward recursively) that each max over the action distribution is achieved at a single action yields the claimed sum-max form expression.']
['E.2 Proof of Theorem 7']
['We now prove our main theorem on the regret of the CFR algorithm.']
['By Lemma E.1, we have e']
['PH h=1 RT h , where for any h ∈ [H] we have']
['µ1:(h−1)(xh−1, ah−1)Rimm,T h (xh)']
['µ1:(h−1)(xh−1, ah−1) max']
['µ† h']
['(·|xh)']
['µt h(·|xh) − µ† h(·|xh), e']
['Lt h(xh, ·)']
[':= e']
['Rimm,T h (xh)']
['µt h(·|xh), Lt h(xh, ·) − e']
['µ† h(·|xh), e']
['Lt h(xh, ·) − Lt h(xh, ·)']
['µ1:(h−1)(xh−1, ah−1)e']
[':=REGRETh']
['µt h(ah|xh) h']
['Lt h(xh, ah) − e']
['Lt h(xh, ah) i']
[':=BIAS1 h']
['X t=1 h e']
['Lt h(xh, ah) − Lt h(xh, ah) i']
[':=BIAS2 h']
['= REGRETh + BIAS1 h + BIAS2 h.']
['Above, the simplification of the BIAS2 h part in (i) uses the fact that the inner max over µ† h(·|xh) and the outer max over µ1:(h−1) are separable and thus can be merged into a single max over µ1:h.']
['We now state three lemmas that bound each term above. Their proofs are deferred to Sections E.3-E.5.']
['Lemma E.2 (Bound on BIAS1 h). For any sequence of opponents’ policies νt']
['∈ Ft−1, using the estimator e']
['Lh in (12), with probability 1 − δ/10, we have']
['BIAS1 h ≤ 2']
['H3XAT ι + HXι, where ι = log(10X/δ).']
['Lemma E.3 (Bound on BIAS2 h). For any sequence of opponents’ policies νt']
['BIAS2 h ≤ 2']
['H3XAT ι + HXAι, where ι = log(10XA/δ).']
['Lemma E.4 (Bound on REGRETh). Choosing η = p']
['XAι/(H3T ), we have that with probability at least']
['1 − δ/10 (over the randomness within the loss estimator e']
['Lt h), H']
['REGRETh ≤ 2']
['H3XAT ι + p']
['HX3A3ι3/(4T ), where ι = log(10XA/δ).']
['Combining Lemma E.2, E.3, and E.4, we obtain the following: Choosing η = p']
['XAι/(H3T ), with probability at least 1 − 3δ/10 ≥ 1 − δ, we have e']
['RT h ≤']
['REGRETh +']
['BIAS1 h +']
['BIAS2 h']
['H3XAT ι + 2HXAι + p']
['HX3A3ι3/(4T ).']
['Additionally, recall the naive bound e']
['µ ∈ Πmax, t ∈ [T ]), we get e']
['HX3A3ι3/4T, HT o']
['HXAι/T + 2XAι/T + p']
['X3A3ι3/(4HT 3), 1 o']
['For T > HXAι, the min above is upper bounded by 9 p']
['HXAι/T. For T ≤ HXAι, the min above is upper bounded by 1 ≤ 9 p']
['HXAι/T. Therefore, we always have e']
['≤ HT · 9 p']
['HXAι/T = 9']
['E.3 Proof of Lemma E.2']
['Rewrite BIAS1 h as']
['BIAS1 h = max']
['1:(h−1)(xh−1, ah−1)µt h(ah|xh) · h']
['µ⋆,h h (ah|xh)']
['1:h(xh, ah)Lt h(xh, ah) − H − h + 1 −']
['X h′=h r t,(h) h′']
['1 n']
['(x t,(h) h , a t,(h) h ) = (xh, ah) o']
[':=e']
['∆ xh t']
['(30)']
['Observe that the random variables e']
['∆xh t satisfy the following:']
['• e']
['∆xh t ≤ H almost surely: e']
['∆xh t ≤']
['· µ⋆,h']
['1:h(xh, ah)Lt h(xh, ah)']
['µt h(ah|xh)µ⋆,h']
['1:(h−1)(xh−1, ah−1)Lt h(xh, ah) ≤ H.']
['Above, the last bound follows from Lemma C.1(a).']
['• E[e']
['∆xh t |Ft−1] = 0, where Ft−1 is the σ-algebra containing all information after iteration t − 1;']
['• The conditional variance E[(e']
['∆xh t )2']
['\x14\x10 e']
['∆xh t']
['Ft−1']
['≤ E']
['\uf8f0']
['· H − h + 1 −']
['≤ H2']
['· Pµ⋆,h']
[',νt']
['(x t,(h) h , a t,(h) h ) = (xh, ah)']
['= H2']
['1:h(xh, ah) · pνt']
['≤A']
['·µ⋆,h']
['1:h−1(xh−1, ah−1) · µt h(ah|xh)pνt']
['A ·']
['1:h(xh).']
['Therefore, we can apply Freedman’s inequality (Lemma A.1) and union bound to get that, for any fixed']
['λ ∈ (0, 1/H], with probability at least 1 − δ/10, the following holds simultaneously for all (h, xh):']
['X t=1 e']
['∆xh t ≤ λH2']
['A']
['µt h(ah|xh)pνt']
['1:h(xh) +']
[', where ι := log(10X/δ) is a log factor. Plugging this bound into (30) yields that, for all h ∈ [H], BIAS1 h = max']
['λH2']
['≤ λH2']
['A · max']
['µ1:h−1(xh−1, ah−1)']
['· max']
['= λH2']
['AT +']
['A max']
['(µ1:(h−1)µunif h )(xh, ah)']
['· Xh.']
['Above, (i) used the fact that']
['(xh,ah)∈Xh×A µ1:h−1(xh−1, ah−1)µt h(ah|xh)pνt']
['1:h(xh) = 1 for any µ ∈ Πmax and any t ∈ [T ] (Lemma C.1(a)), as well as the fact that µ⋆,h h (ah|xh) = µunif h (ah|xh) := 1/A; (ii) used the balancing property of µ⋆,h']
['1:h (Lemma C.4). Combining the bounds for all h ∈ [H], we get that with probability at least 1 − δ/10, H']
['BIAS1 h ≤ λH3']
['Xι']
['Choosing']
['λ = min']
['(r']
['H3AT']
[', 1']
[', we obtain the bound']
['H3XAT ι + HXι.']
['E.4 Proof of Lemma E.3']
['The proof strategy is similar to Lemma E.2. We can rewrite BIAS2 h as']
['BIAS2 h = max']
['H − h + 1 −']
['− µ⋆,h']
[':=∆ xh,ah t']
[', (31) where the last equality used the definition of the loss estimator e']
['Lt h(xh, ah) in (12).']
['Observe that the random variables ∆xh,ah t satisfy the following:']
['• ∆xh,ah t ≤ H almost surely.']
['• E[∆']
['(xh,ah) t |Ft−1] = 0, where Ft−1 is the σ-algebra containing all information after iteration t − 1. This follows as the episode was sampled using µt,(h)']
['= µ⋆,h']
['1:hµt h+1:H, as well as the definition of Lt h(xh, ah) in (10).']
['• The conditional variance E[(∆']
['(xh,ah) t )2']
['\x14\x10']
['∆']
['(xh,ah) t']
['\uf8f0 H − h + 1 −']
['Pµ⋆,h']
['1:h ,νt']
['λ ∈ (0, 1/H], with probability at least 1 − δ/10, the following holds simultaneously for all (h, xh, ah):']
['(xh,ah) t ≤ λH2']
['1:h(xh, ah) ·']
['X t=1 pνt']
[', where ι := log(10XA/δ) is a log factor. Plugging this bound into (31) yields that, for all h ∈ [H], BIAS2 h = max']
['∆xh,ah t']
['· XhA.']
['(xh,ah)∈Xh×A µ1:h(xh, ah)pνt']
['1:h(xh) = 1 for any µ ∈ Πmax and any t ∈ [T ]']
['(Lemma C.1(a)), as well as the balancing property of µ⋆,h']
['BIAS2 h ≤ λH3']
['H3T']
['H3XAT ι + HXAι.']
['E.5 Proof of Lemma E.4']
['Recall that for all (h, xh), we have implemented Line 8 of Algorithm 2 as the Hedge algorithm (Algorithm 3) with learning rate ηµ⋆,h']
['1:h(xh, a) and loss vector n e']
['Lt h(xh, a) o a∈A']
['(cf. (11)). Therefore, applying the standard regret bound for Hedge (Lemma B.1), we get (below a ∈ A is arbitrary) e']
['1:h(xh, a)']
['1:h(xh, ah) · µt h(ah|xh)']
['Lt h(xh, ah)']
['1:h(xh, ah)µt h(ah|xh) ·']
['PH h′=h r t,(h) h′']
['µt h(ah|xh) ·']
['(32)']
['Above, (i) used the form of e']
['Lt h in (12). Plugging this into the definition of REGRETh, we have']
['REGRETh = max']
['µ1:(h−1)(xh−1, ah−1) · log A']
['Ih']
['µ1:(h−1)(xh−1, ah−1) ·']
['IIh']
['(33)']
['We first calculate term Ih. We have']
['Ah']
['· XhA =']
['XhA log A']
[', where (i) follows by splitting the sum over ah and using the fact that µ⋆,h']
['1:h(xh, a) does not depend on a; (ii) follows from the balancing property of µ⋆,h']
['1:h (Lemma C.4).']
['Next, we bound term IIh. We have']
['IIh =']
['2 max']
['µt h(ah|xh) · 1 n']
[':=∆ xh t']
['. (34)']
['The last equality above used the fact that µ⋆,h']
['1:h(xh, ah) does not depend on ah (cf. (5)).']
['Observe that the random variables ∆ xh t satisfy the following:']
['• ∆ xh t ∈ [0, 1] almost surely;']
['• E[∆ xh t |Ft−1] =']
['P ah∈A µ⋆,h']
['1:h(xh, ah) · µt h(ah|xh)pνt']
['1:h(xh), where Ft−1 is the σ-algebra containing all information after iteration t − 1;']
['• The conditional variance Var[∆ xh t |Ft−1] can be bounded as']
['Var h']
['∆ xh t Ft−1 i']
['\x012']
['1:h ×νt']
['· pνt']
['Therefore, we can apply Freedman’s inequality (Lemma A.1) and a union bound to obtain that, for any']
['λ ∈ (0, 1], with probability at least 1 − δ/10, the following holds simultaneously for all (h, xh):']
['∆ xh t −']
['≤ λ ·']
[', where ι := log(10X/δ) is a log factor. Plugging this bound into (34) yields that, for all h ∈ [H], IIh ≤']
['µ1:(h−1)(xh−1, ah−1) · λ']
['(1 + λ) · max']
['(1 + λ)T +']
['Above, (i) used again the fact that µ⋆,h']
['1:h(xh, a) = µ⋆,h']
['1:h(xh, ah) for any a, ah ∈ A; (ii) used the fact that']
['µt h(ah|xh) ≤ 1; (iii) used the fact that']
['(xh,ah)∈Xh×A(µ1:(h−1)µt h)(xh, ah)νt']
['(xh) = 1 for any µ ∈ Πmax and any t ∈ [T ] (Lemma C.1(a)), as well as the balancing property of µ⋆,h']
['Combining the bounds for Ih and IIh, we obtain that']
['REGRETh ≤']
['(Ih + IIh)']
['\x14']
['XhAι']
['2λ']
['ηH3']
['λ · HT +']
[', where we have redefined the log factor ι := log(10XA/δ). Choosing λ = 1, the above can be upper bounded by']
['Further choosing η = p']
['XAι/(H3T ), we obtain the bound']
['F Balanced CFR with regret matching']
['In this section, we consider instantiating Line 8 of Algorithm 2 using the following Regret Matching algorithm:']
['µt+1 h (a|xh) =']
['Rt xh']
['(a)']
['P a′∈A']
['(a′)']
[', where Rt xh']
['(a) := t']
['τ=1']
['µτ h(·|xh), e']
['Lτ h(xh, ·)']
['Lτ h(xh, a) for all a ∈ A.']
['(35)']
['We now present the main theoretical guarantees for Balanced CFR with regret matching. The proof of']
['Theorem F.1 can be found in Section F.1.']
['Theorem F.1 (“Regret” bound for Balanced CFR with Regret Matching). Suppose the max player plays']
['Algorithm 2 where each Rxh is instantiated as the Regret Matching algorithm (35). Then the policies µt achieve the following regret bound with probability at least 1 − δ: e']
[':= max']
['≤ O(']
['H3XA2T ι), where ι = log(10XA/δ) is a log factor. Further, each round plays H episodes against νt']
['(so that the total number of episodes played is HT ).']
['We then have the following corollary directly by the regret-to-Nash conversion (Proposition 1).']
['Corollary F.2 (Learning Nash using Balanced CFR with Regret Matching). Letting both players play']
['Algorithm 2 in a self-play fashion against each other for T rounds, where each Rxh is instantiated as the']
['Regret Matching algorithm (35). Then, for any ε > 0, the average policy (µ, ν) = ( 1']
['PT t=1 νt']
[') achieves NEGap(µ, ν) ≤ ε with probability at least 1 − δ, as long as']
['T ≥ O(H3']
['(XA2']
['+ Y B2']
[')ι/ε2']
['), where ι := log(10(XA + Y B)/δ) is a log factor. The total amount of episodes played is at most']
['2H · T = O(H4']
['F.1 Proof of Theorem F.1']
['The proof is similar as Theorem 7, except for plugging in the regret bound for Regret Matching instead of']
['Hedge.']
['First, by Lemma E.1, we have e']
['RT h ≤ max']
['= REGRETh + BIAS1 h + BIAS2 h, (36) where the definition of e']
['Rimm,T h (xh), Lt h(xh, ah) are at the beginning of Section E.1 and the definition of e']
['Lt h(xh, ah) are given by Algorithm 2.']
['To upper bound BIAS1 h and BIAS2 h, we use the same strategy as the proof of Lemma E.2 and E.3 (whose proofs are independent of the regret minimizer), so that we have the same bound as in Lemma E.2 and E.3: with probability at least 1 − δ/5, we have']
['H3XAT ι + HXι, H']
['H3XAT ι + HXAι, (37) where ι = log(10XA/δ).']
['To upper bound REGRETh, we use the same strategy as the proof of Lemma E.4 as in Section E.5. First, applying the regret bound for Regret Matching (Lemma B.2 & Remark B.3), we get (below a ∈ A is arbitrary, and η > 0 is also arbitrary) e']
['1:h(xh, ah) · Aµ̄t h(ah|xh)']
['A · µ̄t h(ah|xh) ·']
[', (38) where µ̄t h(ah|xh) = (µt h(ah|xh) + (1/A))/2 is a probability distribution over [A]. Comparing the right hand side of Eq. (38) with the right hand side of Eq. (32), we can see that there is only one difference which is']
['A · µ̄t h versus µt h. Plugging this into the definition of REGRETh, we have']
['(39)']
['Comparing Eq. (39) with Eq. (33), we can see that Ih in Eq. (39) is the same as Ih in Eq. (33), and IIh in']
['Eq. (39) and (33) only have one difference which is also A · µ̄t h versus µt h. Using the same argument as in the former proof, we have']
['Ih =']
['XhA']
['Furthermore, using the same argument as in the former proof, we can show that the upper bound of IIh in Eq. (39) is at most A times the upper bound of IIh in Eq. (33). This gives for any λ ∈ (0, 1), with probability at least 1 − δ/10, we have']
['IIh ≤']
['· XhA2']
['(Ih + IIh) ≤']
['XA']
[', Choosing λ = 1 and choosing η = p']
['Xι/(H3T ), with probability at least 1 − δ/10, we obtain the bound']
['H3XA2T ι + p']
['HX3A4ι3/(4T ). (40)']
['This bound is']
['A times larger than the bound of']
['PH h=1 REGRETh as in Lemma E.4.']
['Combining Eq. (36), (37) and (40), we obtain the following: with probability at least 1 − 3δ/10 ≥ 1 − δ, we have e']
['H3XA2T ι + 2HXAι + p']
['HX3A4ι3/(4T ).']
['HX3A4ι3/4T, HT o']
['HXA2ι/T + 2XAι/T + p']
['X3A4ι3/(4HT 3), 1 o']
['For T > HXA2']
['ι, the min above is upper bounded by 9 p']
['HXA2ι/T. For T ≤ HXA2']
['ι, the min above is upper bounded by 1 ≤ 9 p']
['HXA2ι/T. Therefore, we always have e']
['HXA2ι/T = 9']
['H3XA2T ι.']
['G Results for multi-player general-sum games']
['We introduce multi-player general-sum games with imperfect information and show when all the players run Algorithm 1 or Algorithm 2 independently, the average policy is an approximate Coarse Correlated']
['Equilibrium policy.']
['G.1 Multi-player general-sum games']
['Here we define an m-player general-sum IIEFG with tree structure and perfect recall. Our definition is parallel to the POMG formulation for two-player zero-sum IIEFGs in Section 2.']
['Partially observable Markov games We consider finite-horizon, tabular, m-player general-sum Markov']
['Games with partial observability. Formally, it can be described as a POMG(H, S, {Xi}m i=1, {Ai}m i=1, P, {ri}m i=1), where']
['• H is the horizon length;']
['• S =']
['S h∈[H] Sh is the (underlying) state space;']
['• Xi =']
['S h∈[H] Xi,h is the space of infosets for the i-th player with |Xi,h| = Xi,h and Xi :=']
['PH h=1 Xi,h. At any state sh ∈ Sh, the i-th player only observes the infoset xi,h = xi(sh) ∈ Xi,h, where xi : S → Xi is the emission function for the i-th player;']
['• Ai is the action spaces for the i-th player with |Ai| = Ai. For any h, we define the joint action of m players by ah := (a1,h, · · · , am,h) and the set of joint actions by A := A1 × · · · × Am.']
['• P = {p1(·) ∈ ∆(S1)}∪{ph(·|sh, ah) ∈ ∆(Sh+1)}(sh,ah)∈Sh×A, h∈[H−1] are the transition probabilities, where p1(s1) is the probability of the initial state being s1, and ph(sh+1|sh, ah) is the probability of transitting to sh+1 given state-action (sh, ah, bh) at step h;']
['• ri = {ri,h(sh, ah) ∈ [0, 1]}(sh,ah)∈Sh×A are the (random) reward functions with mean ri,h(sh, ah).']
['Policies, value functions A policy for the i-th player is denoted by πi = {πi,h(·|xi,h) ∈ ∆(Ai)}h∈[H],xi,h∈Xi,h']
[', where πi,h(ai,h|xi,h) is the probability of taking action ai,h ∈ Ai at infoset xi,h ∈ Xi,h. A trajectory for the i-th player takes the form (xi,1, ai,1, ri,1, xi,2, . . . , xi,H, ai,H, ri,H ), where ai,h ∼ πi,h(·|xi,h), which depends on both the other (unseen) players’ policy and underlying state transition.']
['We use π to denote the joint policy. Notice although the marginals are πi, π is not necessarily a product policy. When π is indeed a product policy, we have π = π1 × · · · × πm. We also use π−i to denote the joint product policy excluding the i-th player. The overall game value of the i-th player for any joint policy π is denoted by V π i := Eπ hPH h=1 ri,h(sh, ah) i']
['Tree structure and perfect recall As before, we assume']
['• Tree structure: for any h and sh ∈ Sh, there exists a unique history (s1, a1, . . . , sh−1, ah−1) of past states and (joint) actions that leads to h.']
['• Perfect recall: For any h and any infoset xi,h ∈ Xi,h for the i-th player, there exists a unique history']
['(xi,1, ai,1, . . . , xi,h−1, ai,h−1) of past infosets and i-th player’s actions that leads to xi,h.']
['Given above conditions, under any product policy π, the probability of reaching state-action (sh, ah) at step h takes the form']
['Pπ']
['(sh, ah) = p1:h(sh) m']
['Y i=1']
['πi,1:h (xi,h, ai,h), (41) where {sh′ , ah′ }h′≤h−1 are the histories uniquely determined from sh and xi,h′ = xi(sh′ ). We have also defined the sequence-form transition probability as p1:h(sh) := p1(s1)']
['Y h′≤h−1 ph′(sh′+1|sh′, ah′), and the sequence-form policies as']
['πi,1:h (xi,h, ai,h) := h']
['Y h′=1']
['πi,h′ (ai,h′ |xi,h′ ).']
['Regret and CCE Similar as how regret minimization in two-player zero-sum games leads to an approximate Nash equilibrium (Proposition 1), in multi-player general-sum games, regret minimization is known to lead to an approximate NFCCE. Let {πt']
['T t=1 denote a sequence of joint policies (for all players) over T rounds. The regret of the i-th player is defined by']
['RT i := max']
['π† i ∈Πi']
['\x12']
['V']
['π† i ,πt']
['−i i − V πt i']
['. where Πi denotes the set of all possible policies for the i-th player.']
['Using online-to-batch conversion, it is a standard result that sub-linear regret for all the players ensures that the average policy π is an approximate NFCCE (Celli et al., 2019).']
['Proposition G.1 (Regret-to-CCE conversion for multi-player general-sum games). Let the average policy']
['π be defined as playing a policy within {πt']
['T t=1 uniformly at random, then we have']
['CCEGap(π) = maxi∈[m] RT i']
['We include a short justification for this standard result here for completeness.']
['Proof. By definition of π, we have for any i ∈ [m] and π† i ∈ Πi that']
['π† i ,π−i i − V π i =']
['Taking the max over π† i ∈ Πi and i ∈ [m] on both sides yields the desired result.']
['G.2 Proof of Theorem 10']
['It is straightforward to see that the regret guarantees for Balanced OMD (Theorem 4) and Balanced CFR']
['(Theorem 7) also hold in multi-player general-sum games (e.g. by modeling all other players as a single opponent). Therefore, the regret-to-CCE conversion in Proposition G.1 directly implies that, letting π denote the joint policy of playing a uniformly sampled policy within {πt']
['T t=1, we have for Balanced OMD that']
['CCEGap(π) ≤ O maxi∈[m]']
['H3XiAiιT']
['= O s']
['H3']
['\x12 max i∈[m]']
['XiAi']
['ι/T']
[', with probability at least 1−δ, where ι := log(3H']
['Pm i=1 XiAi/δ) is a log factor. Choosing T ≥ e']
['O H3 maxi∈[m] XiAi']
['ι/ε2']
['\x01 ensures that the right-hand side is at most ε. This shows part (a). A similar argument can be done for the']
['Balanced CFR algorithm to show part (b).']
